# Sprint 2 Week 2: Testing Crisis & epti Completion

**Sprint Duration**: 2025-10-28 to 2025-11-04 (1 week, Week 2 of Sprint 2)
**Generated**: 2025-10-28 07:03:03
**Source Backlog**: BACKLOG-2025-10-28-070303.md
**Source STATUS**: STATUS-2025-10-28-065832.md
**Sprint Goal**: ADDRESS TESTING CRISIS, complete epti to 100%, create essential documentation

---

## Sprint 2 Week 2 Emergency Context

### Sprint 2 Week 1 Review
**Achievements**: ðŸŽ‰ Exceptional implementation quality
- âœ… P1-3: agent-loop skills implemented (1,763 lines, 4 skills)
- âœ… P1-4: agent-loop hooks configured (3 hooks)
- âœ… P1-5: epti agent implemented (636 lines)
- âœ… P1-6: epti commands implemented (2,805 lines, 6 commands)
- âœ… 100% of planned P1 items delivered
- âœ… agent-loop reached 100% completion
- âœ… epti reached 60% completion
- âœ… Overall project: 35% â†’ 58% (+23%)

**Critical Failures**: âš ï¸ Zero testing or documentation
- âŒ P2-5: agent-loop testing NOT executed (was planned for Week 1)
- âŒ **5,752 lines of code with ZERO execution evidence**
- âŒ CLAUDE.md still critically outdated (claims "skeleton/placeholder phase")
- âŒ No README documentation (0 of 4 READMEs exist)
- âŒ Cannot confirm plugins actually work

**Sprint 2 Week 1 Grade**: B+ (85%)
- **A+** for implementation quality and scope completion
- **F** for testing (0% executed despite planning)
- **F** for documentation (CLAUDE.md actively harmful)

### Week 2 Emergency Priorities

**CRITICAL REALIZATION**: We are implementing blind. 5,752 lines of code exists with zero evidence it loads, runs, or works. This is **unacceptable technical debt**.

**Week 2 Mission**: STOP IMPLEMENTING, START VERIFYING
1. **TEST IMMEDIATELY** - Verify agent-loop actually works (P0-1)
2. **FIX DOCUMENTATION** - Update CLAUDE.md to stop misleading (P0-2)
3. **COMPLETE epti** - Add skills and hooks to match agent-loop (P1-7, P1-8)
4. **TEST epti** - Verify TDD workflow before declaring complete (P2-6)
5. **DOCUMENT** - Create READMEs so users can actually use marketplace (P2-1, P2-2, P2-3)

**Success = Verification, Not Just Implementation**

---

## Sprint 2 Week 2 Goal

**Primary Objective**: Address the testing crisis and bring marketplace to **verified functional** state

By end of Week 2, we must have:
1. âœ… **agent-loop TESTED** - Manual test with documented results proving it works
2. âœ… **CLAUDE.md FIXED** - Documentation reflects reality (58% not "skeleton")
3. âœ… **epti 100% COMPLETE** - Skills and hooks implemented matching agent-loop
4. âœ… **epti TESTED** - Manual test with documented results proving TDD works
5. âœ… **READMEs CREATED** - Root + 2 plugin READMEs (enable user adoption)
6. âœ… **2 of 3 plugins VERIFIED** - Can demonstrate both workflows confidently

**Completion Target**: Move from 58% to 65% **with testing evidence**

**Success Criteria**: Can install marketplace, execute both workflows, and point to test documentation proving they work.

---

## Sprint Backlog (Week 2)

### DAY 1 (Monday): TESTING & DOCUMENTATION CRISIS

#### MORNING: P0-1 - URGENT Manual Testing for agent-loop âš ï¸
**Work Item**: P0-1 (was P2-5, promoted to CRITICAL)
**Effort**: 2 hours
**Owner**: Developer
**Status**: âš ï¸ **BLOCKING SPRINT COMPLETION**

**WHY CRITICAL**: 2,311 lines of agent-loop code with ZERO execution evidence. Cannot declare "100% complete" without verification.

**Acceptance Criteria**:
- [ ] Test plan created for agent-loop
- [ ] Marketplace loads in Claude Code
- [ ] agent-loop plugin installs successfully
- [ ] /explore command tested - verifies explores without coding
- [ ] /plan command tested - verifies plan document created
- [ ] /code command tested - verifies implementation follows plan
- [ ] /commit command tested - verifies git operations
- [ ] Skills tested - verify invocable and provide value
- [ ] Hooks tested - pre-commit, post-code, commit-msg
- [ ] Test results documented with screenshots/observations
- [ ] Issues filed if discovered

**Test Scenario**: "Add a helper function to calculate business days between two dates"

**Implementation Steps**:
1. Load marketplace in Claude Code environment
2. Install agent-loop plugin
3. Execute /explore on sample project
   - Verify: Agent explores codebase without writing code
   - Verify: Finds relevant files and patterns
   - Document: Observations and any issues
4. Execute /plan
   - Verify: Plan document is created
   - Verify: Plan includes implementation steps
   - Verify: Thinking mode integration works (if applicable)
   - Document: Plan quality and completeness
5. Execute /code
   - Verify: Implementation follows plan
   - Verify: Code is written to correct location
   - Document: Implementation quality
6. Test skills
   - Invoke code-exploration skill
   - Invoke plan-generation skill
   - Invoke verification skill
   - Invoke git-operations skill
   - Document: Skill effectiveness
7. Execute /commit
   - Verify: Git operations execute
   - Verify: Hooks trigger (pre-commit, commit-msg)
   - Document: Hook behavior
8. Create test results document

**Deliverable**: `tests/manual/agent-loop-test-results.md` with complete test evidence

---

#### AFTERNOON: P0-2 - URGENT Update CLAUDE.md âš ï¸
**Work Item**: P0-2 (was P2-7, promoted to CRITICAL)
**Effort**: 1 hour
**Owner**: Developer
**Status**: âš ï¸ **ACTIVELY HARMFUL**

**WHY CRITICAL**: CLAUDE.md claims plugins are in "skeleton/placeholder phase" when agent-loop is 100% complete (2,311 lines) and epti is 60% complete (3,441 lines). This actively misleads AI assistants and developers.

**Acceptance Criteria**:
- [ ] Line 85 "skeleton/placeholder phase" REMOVED
- [ ] Plugin status updated: agent-loop "100% Complete", epti "60% Complete (completing Week 2)", visual-iteration "15% (Sprint 3)"
- [ ] Current Plugins section updated with actual command names
- [ ] agent-loop documented: 4 commands, 4 skills, 3 hooks, 2,311 lines
- [ ] epti documented: agent + 6 commands implemented (3,441 lines), skills/hooks in progress
- [ ] visual-iteration documented: structure only, no implementation yet
- [ ] Development Workflow section updated with current practices
- [ ] All paths and references verified correct

**Implementation Steps**:
1. Read current CLAUDE.md (lines 84-94 especially)
2. Update "Current Plugins" section:
   ```markdown
   ### Current Plugins

   1. **agent-loop** (v0.1.0)
      - Purpose: Disciplined Software Engineering Loop (Explore â†’ Plan â†’ Code â†’ Commit)
      - Status: âœ… 100% Complete (2,311 lines: agent 206, commands 342, skills 1,763)
      - Components:
        - Agent: workflow-agent.md - 4-stage workflow with thinking mode integration
        - Commands: /explore, /plan, /code, /commit (4 commands, 342 lines)
        - Skills: code-exploration, plan-generation, verification, git-operations (4 skills, 1,763 lines)
        - Hooks: pre-commit (plan check), post-code (test reminder), commit-msg (conventional commits)
      - Testing: Manual testing in progress (Week 2)

   2. **epti** (v0.1.0)
      - Purpose: Test-Driven Development Workflow (Write Tests â†’ Verify Fail â†’ Implement â†’ Iterate â†’ Commit)
      - Status: ðŸš§ 60% Complete (completing Week 2, target 100%)
      - Components:
        - Agent: tdd-agent.md - 6-stage TDD workflow (636 lines)
        - Commands: /write-tests, /verify-fail, /commit-tests, /implement, /iterate, /commit-code (6 commands, 2,805 lines)
        - Skills: In progress (Week 2)
        - Hooks: In progress (Week 2)
      - Testing: Planned for Week 2

   3. **visual-iteration** (v0.1.0)
      - Purpose: Visual Design Iteration (Write Code â†’ Screenshot â†’ Compare â†’ Iterate)
      - Status: ðŸ—ï¸ 15% Structure Only (Sprint 3 target)
      - Components: Plugin manifest and structure created, no implementation yet
   ```
3. Remove all "skeleton/placeholder" language throughout file
4. Update Development Workflow section with Sprint 2 practices
5. Verify all file paths are correct
6. Commit changes with message: "docs: update CLAUDE.md to reflect Sprint 2 Week 1 progress"

**Deliverable**: Updated CLAUDE.md reflecting 58% completion and actual plugin states

---

### DAY 2-3 (Tuesday-Wednesday): Complete epti Implementation

#### DAY 2: P1-7 - Implement epti Skills
**Work Item**: P1-7
**Effort**: 3-4 hours
**Owner**: Developer
**Dependencies**: None (epti agent already complete)

**Acceptance Criteria**:
- [ ] test-gen.md created - test generation from requirements
- [ ] test-exec.md created - test execution and result parsing
- [ ] implement-safe.md created - implementation with test-protection
- [ ] overfit-detect.md created - overfitting detection via subagents
- [ ] framework-detect.md created - test framework detection
- [ ] All skills in plugins/epti/skills/
- [ ] Skills documented with usage examples
- [ ] Skills work with pytest, jest, go test minimum
- [ ] .gitkeep removed from skills directory

**Implementation Steps**:

1. Create `plugins/epti/skills/test-gen.md` (~300-350 lines):
   ```markdown
   # Test Generation Skill

   You are generating tests from requirements using test-first principles.

   ## Mission
   Generate comprehensive tests that verify expected behavior BEFORE implementation exists.

   ## Process
   1. Analyze requirements thoroughly
   2. Identify expected behaviors and edge cases
   3. Generate tests for each behavior
   4. Ensure tests are independent and isolated
   5. DO NOT write any implementation code

   ## Framework-Specific Templates
   [pytest examples]
   [jest examples]
   [go test examples]

   ## Anti-Patterns to Avoid
   - Mock implementations during test generation
   - Tests that assume implementation details
   - Incomplete edge case coverage

   ## Verification
   Use subagents to verify tests are comprehensive
   ```

2. Create `plugins/epti/skills/test-exec.md` (~300-350 lines):
   ```markdown
   # Test Execution Skill

   You are executing tests and parsing results to verify behavior.

   ## Mission
   Run test suite, parse results, provide clear feedback on pass/fail status.

   ## Test Runner Detection
   [Framework detection logic]
   - pytest: Look for pytest.ini, setup.py, *_test.py
   - jest: Look for jest.config.js, package.json, *.test.js
   - go test: Look for go.mod, *_test.go

   ## Execution Commands
   - pytest: `pytest -v`
   - jest: `npm test` or `npx jest`
   - go test: `go test -v ./...`

   ## Result Parsing
   [How to parse output, count passes/failures, extract error messages]

   ## Special Modes
   - --verify-fail: Confirm tests fail (used after test writing)
   - --block-on-fail: Exit non-zero if any test fails (used in hooks)
   ```

3. Create `plugins/epti/skills/implement-safe.md` (~300-350 lines):
   ```markdown
   # Safe Implementation Skill

   You are implementing code to pass tests WITHOUT modifying tests.

   ## Mission
   Write simplest implementation that makes tests pass. Protect test integrity.

   ## Rules
   1. NEVER modify test files during implementation
   2. Focus on simplest solution that passes tests
   3. Implement only what tests require
   4. Use iteration to refine, not wholesale rewrites

   ## Process
   1. Review test requirements
   2. Identify behaviors tests expect
   3. Implement minimal solution
   4. Run tests
   5. Iterate if needed

   ## Red Flags
   [Warning signs of test modification]
   [Warning signs of over-engineering]

   ## Verification
   Use git diff to verify tests unchanged
   ```

4. Create `plugins/epti/skills/overfit-detect.md` (~300-400 lines):
   ```markdown
   # Overfitting Detection Skill

   You are detecting when implementation overfits to tests rather than solving actual problem.

   ## Mission
   Use subagents to verify implementation solves the real problem, not just test cases.

   ## Overfitting Definition
   Implementation that passes tests but doesn't solve the underlying problem.

   ## Detection Strategy
   1. Spawn subagent without access to tests
   2. Ask subagent: "Does this implementation solve [requirement]?"
   3. Compare subagent analysis to test requirements
   4. Flag discrepancies as potential overfitting

   ## Examples of Overfitting
   - Hardcoded return values matching test expectations
   - If-statements checking for specific test inputs
   - Implementation that works for test cases but fails for similar inputs

   ## Verification Process
   [Detailed subagent coordination]
   [Questions to ask]
   [Red flags to watch for]

   ## Remediation
   If overfitting detected, refactor implementation to be more general
   ```

5. Create `plugins/epti/skills/framework-detect.md` (~300-350 lines):
   ```markdown
   # Test Framework Detection Skill

   You are detecting test framework from project structure.

   ## Mission
   Automatically identify test framework to use appropriate commands and conventions.

   ## Detection Logic

   ### Python (pytest)
   - Files: pytest.ini, setup.py (with pytest in install_requires), setup.cfg
   - Patterns: *_test.py, test_*.py
   - Confidence: HIGH if pytest.ini exists

   ### JavaScript (jest)
   - Files: jest.config.js, jest.config.ts, package.json (with jest in devDependencies)
   - Patterns: *.test.js, *.spec.js, __tests__/ directory
   - Confidence: HIGH if jest.config.js exists

   ### Go (go test)
   - Files: go.mod, go.sum
   - Patterns: *_test.go
   - Confidence: HIGH if go.mod exists

   ### Java (junit)
   - Files: pom.xml, build.gradle
   - Patterns: *Test.java, src/test/java/
   - Confidence: HIGH if pom.xml with junit dependency

   ### Ruby (rspec)
   - Files: Gemfile (with rspec), .rspec
   - Patterns: *_spec.rb, spec/ directory
   - Confidence: HIGH if .rspec exists

   ## Fallback
   If no framework detected, prompt user for manual specification

   ## Multi-Framework Projects
   [How to handle projects with multiple test frameworks]
   ```

**Deliverable**: 5 comprehensive skill files (~1,500-1,800 lines total), .gitkeep removed

---

#### DAY 3: P1-8 - Configure epti Hooks
**Work Item**: P1-8
**Effort**: 2-3 hours
**Owner**: Developer
**Dependencies**: P1-7 complete (test-exec skill needed)

**Acceptance Criteria**:
- [ ] hooks.json contains 3 test automation hooks
- [ ] Hook: afterTestWrite - verifies tests fail before implementation
- [ ] Hook: afterCodeChange - runs test suite automatically
- [ ] Hook: beforeCommit - blocks commits if tests fail
- [ ] Hooks detect test framework automatically
- [ ] Hook failures provide clear error messages
- [ ] Hooks documented
- [ ] Hooks tested and confirmed working

**Implementation Steps**:

1. Update `plugins/epti/hooks/hooks.json`:
   ```json
   [
     {
       "event": "afterTestWrite",
       "command": "bash -c 'cd \"$PROJECT_ROOT\" && ./scripts/run-tests.sh --verify-fail'",
       "description": "Verify tests fail before implementation (proves they test the right thing)"
     },
     {
       "event": "afterCodeChange",
       "command": "bash -c 'cd \"$PROJECT_ROOT\" && ./scripts/run-tests.sh'",
       "description": "Run tests after code changes to provide immediate feedback"
     },
     {
       "event": "beforeCommit",
       "command": "bash -c 'cd \"$PROJECT_ROOT\" && ./scripts/run-tests.sh --block-on-fail'",
       "description": "Prevent commit if tests fail (enforce TDD discipline)"
     }
   ]
   ```

2. Create `scripts/run-tests.sh`:
   ```bash
   #!/usr/bin/env bash
   # Test runner script with framework auto-detection

   set -e

   MODE="${1:-normal}"  # normal, verify-fail, block-on-fail

   # Detect test framework
   if [ -f pytest.ini ] || [ -f setup.py ]; then
       FRAMEWORK="pytest"
       CMD="pytest -v"
   elif [ -f jest.config.js ] || [ -f jest.config.ts ]; then
       FRAMEWORK="jest"
       CMD="npm test"
   elif [ -f go.mod ]; then
       FRAMEWORK="go"
       CMD="go test -v ./..."
   else
       echo "Error: No test framework detected"
       exit 1
   fi

   echo "Running tests with $FRAMEWORK..."

   # Run tests
   if [ "$MODE" = "verify-fail" ]; then
       # Expect failures
       if $CMD; then
           echo "ERROR: Tests passed but should fail (no implementation exists yet)"
           exit 1
       else
           echo "SUCCESS: Tests failed as expected"
           exit 0
       fi
   elif [ "$MODE" = "block-on-fail" ]; then
       # Block on any failure
       $CMD || (echo "ERROR: Cannot commit with failing tests" && exit 1)
   else
       # Normal run
       $CMD
   fi
   ```

3. Make script executable:
   ```bash
   chmod +x scripts/run-tests.sh
   ```

4. Test hooks with sample project:
   - Create sample test that fails
   - Trigger afterTestWrite hook
   - Verify hook runs and confirms failure
   - Create implementation
   - Trigger afterCodeChange hook
   - Verify tests run automatically
   - Attempt commit with failing test
   - Verify beforeCommit hook blocks

5. Document hook behavior in plugin README (done in P2-3)

**Deliverable**: hooks.json configured with 3 hooks, run-tests.sh script created, epti plugin 100% COMPLETE

---

### DAY 4-5 (Thursday-Friday): Testing & Verification

#### DAY 4-5: P2-6 - Manual Testing Protocol for epti
**Work Item**: P2-6
**Effort**: 2 hours
**Owner**: Developer
**Dependencies**: P1-7, P1-8 complete (epti 100%)
**Status**: âš ï¸ **CRITICAL - VERIFY BEFORE DECLARING COMPLETE**

**WHY CRITICAL**: epti will have 3,441 lines (agent + commands) + ~1,800 lines (skills) + hooks = ~5,300 total lines with zero execution evidence. Must verify TDD workflow actually works.

**Acceptance Criteria**:
- [ ] Test plan created for epti
- [ ] Manual test executed: full TDD cycle
- [ ] Verify tests written before implementation (Stage 1)
- [ ] Verify tests fail before implementation (Stage 2)
- [ ] Verify tests committed separately (Stage 3)
- [ ] Verify iteration until tests pass (Stage 4-5)
- [ ] Verify implementation committed separately (Stage 6)
- [ ] Test framework detection verified (pytest AND jest)
- [ ] Hooks tested (afterTestWrite, afterCodeChange, beforeCommit)
- [ ] Test results documented with examples
- [ ] Issues filed if discovered

**Test Scenario 1** (Python/pytest): "Add email and password validation to API endpoint"

**Implementation Steps (Python)**:
1. Load marketplace, install epti
2. Run /write-tests
   - Provide requirement: "API endpoint must validate email format and password length (min 8 chars)"
   - Verify: Tests generated WITHOUT implementation code
   - Verify: Tests check email format (valid/invalid cases)
   - Verify: Tests check password length (boundary cases)
   - Document: Test quality and completeness
3. Run /verify-fail
   - Verify: Tests execute and FAIL (no validation exists)
   - Verify: Failure messages are clear
   - Verify: Hook afterTestWrite triggers and confirms failure
   - Document: Failure output
4. Run /commit-tests
   - Verify: Only test files are staged
   - Verify: Commit message describes tests
   - Verify: Git log shows test commit
   - Document: Commit behavior
5. Run /implement
   - Verify: Validation code written
   - Verify: Tests NOT modified
   - Verify: Hook afterCodeChange triggers and runs tests
   - Document: Implementation approach
6. Run /iterate
   - Verify: Tests run automatically
   - If failures: adjust implementation (NOT tests)
   - Verify: Overfitting detection runs via subagent
   - Continue until all tests pass
   - Document: Iteration process and subagent feedback
7. Run /commit-code
   - Verify: Only implementation files staged
   - Verify: Tests still pass
   - Verify: Hook beforeCommit allows commit (tests passing)
   - Verify: Git log shows separate commits (tests, then implementation)
   - Document: Final commit behavior

**Test Scenario 2** (JavaScript/jest): "Add debounce utility function"

**Implementation Steps (JavaScript)**:
- Repeat above process with JavaScript project
- Verify framework detection identifies jest
- Verify jest commands used instead of pytest
- Document any framework-specific issues

**Deliverable**: `tests/manual/epti-test-results.md` with complete test evidence for BOTH frameworks

---

### DAY 6-7 (Saturday-Sunday): Documentation Sprint

#### DAY 6: P2-2 & P2-3 - Plugin READMEs
**Work Items**: P2-2 (agent-loop README), P2-3 (epti README)
**Effort**: 4 hours total (2 hours each)
**Owner**: Developer
**Dependencies**: P0-1 complete (agent-loop tested), P2-6 complete (epti tested)

**P2-2: agent-loop README Acceptance Criteria**:
- [ ] README.md created in plugins/agent-loop/
- [ ] Plugin purpose and when to use it
- [ ] Workflow stages explained (Explore â†’ Plan â†’ Code â†’ Commit)
- [ ] Commands documented (/explore, /plan, /code, /commit)
- [ ] Skills documented (code-exploration, plan-generation, verification, git-operations)
- [ ] Hooks documented (pre-commit, post-code, commit-msg)
- [ ] Complete workflow example end-to-end
- [ ] Tips for best results
- [ ] Reference to test results from P0-1

**P2-3: epti README Acceptance Criteria**:
- [ ] README.md created in plugins/epti/
- [ ] TDD workflow explained clearly
- [ ] Commands documented (/write-tests, /verify-fail, /commit-tests, /implement, /iterate, /commit-code)
- [ ] Skills documented (test-gen, test-exec, implement-safe, overfit-detect, framework-detect)
- [ ] Hooks documented (afterTestWrite, afterCodeChange, beforeCommit)
- [ ] Test-first discipline explained (why tests fail first)
- [ ] Supported frameworks listed (pytest, jest, go test minimum)
- [ ] Complete TDD cycle example
- [ ] Tips for preventing overfitting
- [ ] Reference to test results from P2-6

**Implementation Steps**:

1. Create `plugins/agent-loop/README.md`:
   ```markdown
   # agent-loop Plugin

   **Status**: âœ… 100% Complete | **Version**: 0.1.0

   Disciplined software engineering workflow: Explore â†’ Plan â†’ Code â†’ Commit

   ## When to Use
   - Implementing new features requiring exploration
   - Complex refactoring across multiple files
   - Architectural changes
   - Any problem where understanding before coding is critical

   ## Workflow Stages

   ### 1. Explore (`/explore`)
   Investigate codebase, understand context, identify patterns.
   - No code written, only analysis
   - Uses code-exploration skill for deep investigation
   - Identifies relevant files, dependencies, patterns

   ### 2. Plan (`/plan`)
   Create implementation plan with thinking mode integration.
   - Uses plan-generation skill
   - Thinking modes: think, think hard, think harder, ultrathink
   - Creates PLAN.md document

   ### 3. Code (`/code`)
   Implement following the plan.
   - References plan for guidance
   - Uses verification skill to check plan alignment
   - Iterative implementation

   ### 4. Commit (`/commit`)
   Finalize with git operations.
   - Uses git-operations skill
   - Hooks enforce workflow discipline
   - Conventional commit messages

   ## Available Commands
   [Command details with examples]

   ## Skills
   [Skill details with parameters]

   ## Hooks
   [Hook details with behavior]

   ## Complete Example
   [End-to-end example from P0-1 test results]

   ## Tips
   - Don't skip exploration - fresh eyes find patterns
   - Use appropriate thinking mode for plan complexity
   - Let plan guide implementation (don't freelance)
   - Hooks will keep you disciplined

   ## Testing
   See [test results](../../tests/manual/agent-loop-test-results.md) for verified functionality.
   ```

2. Create `plugins/epti/README.md`:
   ```markdown
   # epti Plugin

   **Status**: âœ… 100% Complete | **Version**: 0.1.0

   Test-Driven Development with strict discipline: Write Tests â†’ Verify Fail â†’ Implement â†’ Iterate â†’ Commit

   ## When to Use
   - Implementing features with clear requirements
   - Bug fixes that can be captured in tests
   - Refactoring with safety net
   - Any code that can be verified with tests

   ## TDD Workflow

   ### Why Tests First?
   Tests failing first proves they test the right thing. If tests pass before implementation, they're testing nothing.

   ### The Cycle
   1. **Write Tests** (`/write-tests`) - Define expected behavior
   2. **Verify Fail** (`/verify-fail`) - Confirm tests fail (no implementation yet)
   3. **Commit Tests** (`/commit-tests`) - Save tests separately
   4. **Implement** (`/implement`) - Write code to pass tests
   5. **Iterate** (`/iterate`) - Run tests, refine code (NOT tests)
   6. **Commit Code** (`/commit-code`) - Save implementation separately

   ## Available Commands
   [Command details with examples]

   ## Skills
   [Skill details: test-gen, test-exec, implement-safe, overfit-detect, framework-detect]

   ## Hooks
   - **afterTestWrite**: Verifies tests fail (proves they test something)
   - **afterCodeChange**: Runs tests automatically (immediate feedback)
   - **beforeCommit**: Blocks commit if tests fail (enforces discipline)

   ## Supported Test Frameworks
   - **Python**: pytest (auto-detected via pytest.ini, setup.py)
   - **JavaScript**: jest (auto-detected via jest.config.js, package.json)
   - **Go**: go test (auto-detected via go.mod)

   ## Complete Example
   [End-to-end TDD example from P2-6 test results]

   ## Tips
   - Tests failing first is GOOD (proves they work)
   - NEVER modify tests during implementation
   - Use iteration to refine, not wholesale rewrites
   - Watch for overfitting (skill will detect)
   - Separate commits keep history clean

   ## Testing
   See [test results](../../tests/manual/epti-test-results.md) for verified functionality.
   ```

**Deliverable**: Both plugin READMEs created with test evidence links

---

#### DAY 7: P2-1 - Root README
**Work Item**: P2-1
**Effort**: 2-3 hours
**Owner**: Developer
**Dependencies**: P2-2, P2-3 complete (plugin READMEs for linking)

**Acceptance Criteria**:
- [ ] README.md created at repository root
- [ ] Marketplace purpose and overview
- [ ] Installation instructions for Claude Code
- [ ] Plugin overview (all 3 plugins with status)
- [ ] Quick start examples
- [ ] Links to plugin READMEs
- [ ] Requirements section
- [ ] License and author info (MIT, Brandon Fryslie)
- [ ] Troubleshooting section

**Implementation Steps**:

1. Create root `README.md`:
   ```markdown
   # Claude Code Plugin Marketplace

   **Owner**: Brandon Fryslie | **License**: MIT | **Status**: 58% Complete (2 of 3 plugins functional)

   Personal marketplace of specialized workflow plugins for Claude Code.

   ## Overview

   This marketplace provides three workflow-optimized plugins:
   - **agent-loop**: Disciplined engineering (Explore â†’ Plan â†’ Code â†’ Commit)
   - **epti**: Test-driven development (Tests First, Always)
   - **visual-iteration**: Visual design iteration (coming Sprint 3)

   ## Plugins

   ### âœ… agent-loop - Disciplined Engineering Workflow
   **Status**: 100% Complete | **Lines**: 2,311 | [ðŸ“– Documentation](plugins/agent-loop/README.md)

   Systematic approach to software engineering: explore first, plan second, code third, commit last.

   **Commands**: `/explore`, `/plan`, `/code`, `/commit`
   **When to use**: New features, refactoring, architectural changes

   ---

   ### âœ… epti - Test-Driven Development
   **Status**: 100% Complete | **Lines**: ~5,300 | [ðŸ“– Documentation](plugins/epti/README.md)

   Strict TDD workflow: tests before implementation, iteration until pass, separate commits.

   **Commands**: `/write-tests`, `/verify-fail`, `/commit-tests`, `/implement`, `/iterate`, `/commit-code`
   **When to use**: Feature development, bug fixes, refactoring with safety

   ---

   ### ðŸš§ visual-iteration - Visual Design Iteration
   **Status**: In Development (Sprint 3) | [ðŸ“– Documentation](plugins/visual-iteration/README.md)

   Visual design workflow: implement from mock, screenshot, compare, iterate.

   **Commands**: Coming Sprint 3
   **When to use**: UI implementation, design matching, visual refinement

   ## Installation

   ### Prerequisites
   - Claude Code (latest version)
   - Git (for agent-loop and epti git operations)
   - Test framework (for epti: pytest, jest, or go test)

   ### Load Marketplace
   1. Open Claude Code
   2. Navigate to Plugin Marketplace
   3. Load from URL or local path: `/path/to/loom99-claude-marketplace`
   4. Marketplace appears with 3 plugins

   ### Install Plugin
   1. Browse marketplace
   2. Select desired plugin (agent-loop or epti)
   3. Click "Install"
   4. Plugin commands become available (e.g., `/explore`, `/write-tests`)

   ## Quick Start

   ### Example 1: agent-loop Workflow
   ```
   Task: Add a date utility function

   /explore        â†’ Investigate project structure
   /plan          â†’ Create implementation plan
   /code          â†’ Implement function
   /commit        â†’ Finalize with git
   ```

   ### Example 2: epti TDD Workflow
   ```
   Task: Add email validation

   /write-tests   â†’ Write validation tests (NO implementation)
   /verify-fail   â†’ Confirm tests fail
   /commit-tests  â†’ Commit tests only
   /implement     â†’ Write validation code
   /iterate       â†’ Run tests, refine code
   /commit-code   â†’ Commit implementation
   ```

   ## Requirements
   - **Claude Code**: Latest version
   - **Git**: For commit operations (agent-loop, epti)
   - **Test Framework**: For epti (pytest, jest, or go test)
   - **MCP Servers**: Optional for visual-iteration (browser-tools or Playwright)

   ## Troubleshooting

   ### Marketplace won't load
   - Verify `.claude-plugin/marketplace.json` exists
   - Check JSON syntax: `jq . .claude-plugin/marketplace.json`
   - Ensure all plugin.json files are valid

   ### Plugin won't install
   - Check plugin.json path references
   - Verify all referenced files exist
   - Check for .gitkeep files in empty directories

   ### Commands don't appear
   - Verify plugin installed successfully
   - Check plugin.json commands array
   - Restart Claude Code

   ### Hooks don't trigger
   - Verify hooks.json is valid JSON
   - Check hook commands are executable
   - Review hook event names

   ## Development

   See [CLAUDE.md](CLAUDE.md) for development guidelines and project structure.

   ## Testing

   All plugins have manual test results:
   - [agent-loop test results](tests/manual/agent-loop-test-results.md)
   - [epti test results](tests/manual/epti-test-results.md)

   ## License

   MIT License - Brandon Fryslie

   ## Status

   - **Overall**: 58% complete (Sprint 2 Week 1 â†’ Week 2)
   - **agent-loop**: 100% complete, tested âœ…
   - **epti**: 100% complete, tested âœ…
   - **visual-iteration**: 15% (Sprint 3 target) ðŸš§
   ```

**Deliverable**: Root README.md with complete marketplace guide

---

## Sprint Metrics

### Velocity Target
- **Work Items**: 7 items (2 P0, 2 P1, 3 P2)
- **Estimated Hours**: 15-20 hours
- **Success Rate**: 100% of planned items
- **Quality Gate**: ALL items must have test/verification evidence

### Daily Progress Tracking

| Day | Focus | Items | Hours | Deliverables |
|-----|-------|-------|-------|--------------|
| 1 (Mon) | Testing & Docs Emergency | P0-1, P0-2 | 3 | agent-loop tested, CLAUDE.md fixed |
| 2 (Tue) | epti Skills | P1-7 | 3-4 | 5 skills implemented |
| 3 (Wed) | epti Hooks | P1-8 | 2-3 | hooks.json, run-tests.sh, epti 100% |
| 4-5 (Thu-Fri) | epti Testing | P2-6 | 2 | epti verified with 2 frameworks |
| 6 (Sat) | Plugin Docs | P2-2, P2-3 | 4 | Both plugin READMEs |
| 7 (Sun) | Root Docs | P2-1 | 2-3 | Root README |

**Total**: 16-20 hours over 7 days

### Success Metrics
- [ ] P0-1: agent-loop tested (EVIDENCE: test results document)
- [ ] P0-2: CLAUDE.md fixed (EVIDENCE: accurate plugin status)
- [ ] P1-7: epti skills complete (EVIDENCE: 5 skill files)
- [ ] P1-8: epti hooks complete (EVIDENCE: hooks.json + script)
- [ ] P2-6: epti tested (EVIDENCE: test results for 2 frameworks)
- [ ] P2-2, P2-3: Plugin docs (EVIDENCE: 2 READMEs)
- [ ] P2-1: Root docs (EVIDENCE: root README)

---

## Risk Management

### Critical Risks

1. **Testing Reveals Fundamental Issues** âš ï¸
   - **Impact**: VERY HIGH - May require rework
   - **Probability**: MEDIUM - 5,752 lines untested
   - **Mitigation**: Test early (Day 1), allow time for fixes
   - **Contingency**: Extend sprint if critical issues found

2. **Hooks Don't Work as Expected**
   - **Impact**: MEDIUM - epti automation limited
   - **Probability**: MEDIUM - Hook execution environment unclear
   - **Mitigation**: Test hooks thoroughly in P1-8
   - **Contingency**: Document hooks as optional, focus on commands

3. **Framework Detection Incomplete**
   - **Impact**: LOW - Users can manually specify
   - **Probability**: LOW - Detection logic straightforward
   - **Mitigation**: Test with pytest, jest, go test
   - **Contingency**: Add manual framework specification

### Mitigation Strategies

- **Daily Testing**: Test as you build (P1-8 hooks tested immediately)
- **Early Validation**: P0-1 on Day 1 catches issues early
- **Documentation Buffer**: READMEs at end allow adjustment for discoveries
- **Test Evidence Required**: Cannot mark "complete" without test results

---

## Definition of Done

### Work Item "Done" Criteria
- [ ] Code/content written and committed
- [ ] Acceptance criteria all checked
- [ ] **Manual testing completed** (where applicable) âš¡ NEW
- [ ] **Test results documented** (where applicable) âš¡ NEW
- [ ] No errors in execution
- [ ] Documented (inline or README)
- [ ] .gitkeep files removed if directory populated

### Sprint "Done" Criteria
- [ ] Sprint goal achieved (testing crisis resolved, epti 100%, docs created)
- [ ] All 7 work items completed with evidence
- [ ] Both plugins manually tested with documented results âš¡ CRITICAL
- [ ] Both plugins have README documentation
- [ ] Root README created
- [ ] CLAUDE.md updated and accurate
- [ ] No untested code remains (zero-tolerance for blind implementation)
- [ ] Sprint retrospective held
- [ ] Sprint 3 planned

---

## Testing Strategy

### Manual Testing Requirements

**NON-NEGOTIABLE**: Every plugin must be manually tested before declared "complete".

**Test Documentation Template**:
```markdown
# Test Results: [Plugin Name]

**Date**: YYYY-MM-DD
**Tester**: [Name]
**Plugin Version**: [Version]
**Environment**: Claude Code [version]

## Test Scenario
[Description of what was tested]

## Test Steps
1. [Action taken]
   - Expected: [what should happen]
   - Actual: [what did happen]
   - Result: âœ… Pass / âŒ Fail
   - Notes: [observations]

## Issues Discovered
- [Issue 1 with severity]
- [Issue 2 with severity]

## Overall Assessment
- Functionality: [rating/description]
- Usability: [rating/description]
- Documentation alignment: [rating/description]

## Conclusion
[Pass/Fail with summary]
```

### Test Coverage Requirements

**agent-loop** (P0-1):
- âœ… Marketplace loads
- âœ… Plugin installs
- âœ… All 4 commands execute
- âœ… All 4 skills invocable
- âœ… All 3 hooks trigger correctly
- âœ… End-to-end workflow completes

**epti** (P2-6):
- âœ… Marketplace loads
- âœ… Plugin installs
- âœ… All 6 commands execute
- âœ… All 5 skills work correctly
- âœ… All 3 hooks trigger correctly
- âœ… Framework detection works (pytest AND jest)
- âœ… End-to-end TDD workflow completes
- âœ… Tests commit separately from implementation
- âœ… Overfitting detection runs

---

## Handoff to Sprint 3

### Completed Deliverables (Week 2)
- âœ… agent-loop TESTED and VERIFIED working
- âœ… CLAUDE.md FIXED and accurate
- âœ… epti 100% COMPLETE (agent, commands, skills, hooks)
- âœ… epti TESTED and VERIFIED working (2 frameworks)
- âœ… Both plugins documented (plugin READMEs)
- âœ… Root README created (installation guide)
- âœ… 2 of 3 plugins demonstrable with evidence
- âœ… Overall completion: 58% â†’ 65% **with testing**

### Ready for Sprint 3
- **P1-9**: Implement visual-iteration agent
- **P1-10**: Configure MCP servers (browser-tools/Playwright)
- **P1-11**: Implement visual-iteration commands
- **P1-12**: Implement visual-iteration skills
- **P2-4**: Create visual-iteration README
- **P2-8**: Validation scripts (optional)
- **P2-9**: Integration tests (optional)

### Lessons Learned (Week 2)
- Document in retrospective:
  - Impact of testing crisis discovery
  - Importance of continuous verification
  - CLAUDE.md drift prevention strategies
  - Testing-first vs implementation-first tradeoffs

### Recommendations for Sprint 3
- **Test as you build**: Don't accumulate untested code
- **Update docs continuously**: Prevent CLAUDE.md drift
- **Manual testing protocol**: Make it standard for all plugins
- **MCP research early**: Know what's available before implementing
- **Quality over speed**: Extend sprint rather than skip verification

---

## Success Indicators

### Sprint 2 Week 2 is successful if:
1. âœ… P0-1: agent-loop tested with documented results (CRITICAL)
2. âœ… P0-2: CLAUDE.md reflects reality (CRITICAL)
3. âœ… P1-7, P1-8: epti 100% complete (matching agent-loop)
4. âœ… P2-6: epti tested with documented results (CRITICAL)
5. âœ… P2-1, P2-2, P2-3: All essential READMEs created
6. âœ… 2 of 3 plugins demonstrable with confidence
7. âœ… Testing crisis resolved (zero untested code)
8. âœ… Documentation crisis resolved (accurate, current, complete)

### Sprint 2 Week 2 exceeds expectations if:
- Framework detection works for 5+ frameworks (not just 3)
- Test results include video demonstrations
- Additional frameworks tested beyond pytest/jest
- Validation script (P2-8) also completed
- Zero issues discovered in testing (perfect implementation)

---

## Notes

- **Quality Gate Enforced**: No "complete" without test evidence
- **Documentation Discipline**: Update docs as you go, not at end
- **Test-First for Testing**: Ironic but true - test the plugins that test
- **CLAUDE.md is Sacred**: Accuracy critical for AI assistant effectiveness
- **Manual > Automated**: At this stage, manual testing provides more value
- **Evidence Required**: Screenshots, observations, test results - all documented
- **Sprint 2 Week 1 Vindication**: Excellent implementation, now prove it works

---

## Appendix: Quick Reference

### File Locations
- **agent-loop test results**: `tests/manual/agent-loop-test-results.md`
- **epti skills**: `plugins/epti/skills/` (5 files)
- **epti hooks**: `plugins/epti/hooks/hooks.json`
- **Test runner script**: `scripts/run-tests.sh`
- **epti test results**: `tests/manual/epti-test-results.md`
- **agent-loop README**: `plugins/agent-loop/README.md`
- **epti README**: `plugins/epti/README.md`
- **Root README**: `README.md`
- **CLAUDE.md**: `CLAUDE.md` (updated)

### Command Checklist
When testing a command:
- [ ] Command loads without error
- [ ] Command provides expected guidance
- [ ] Command references workflow properly
- [ ] Command transitions to next stage
- [ ] Document observations
- [ ] File issues if broken

### Testing Checklist
Before marking plugin "complete":
- [ ] Plugin installs successfully
- [ ] All commands execute
- [ ] All skills work correctly
- [ ] All hooks trigger properly
- [ ] End-to-end workflow completes
- [ ] Test results documented
- [ ] Issues filed for any problems
- [ ] README references test results

---

## File Provenance

**Source STATUS Report**: STATUS-2025-10-28-065832.md (generated 2025-10-28 06:58:32)
**Source Backlog**: BACKLOG-2025-10-28-070303.md (generated 2025-10-28 07:03:03)
**Sprint Plan Version**: 2 (Week 2 focus)
**Generated By**: status-planner agent
**Generation Timestamp**: 2025-10-28 07:03:03

**Supersedes**:
- SPRINT-02-2025-10-28-063531.md (original Sprint 2 plan, pre-Week 1 completion)

**Context**:
- Sprint 2 Week 1 complete: 4 of 4 P1 items delivered
- Testing crisis discovered: 5,752 lines untested
- Documentation crisis discovered: CLAUDE.md outdated
- Week 2 mission: VERIFY, COMPLETE, DOCUMENT
